{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMINAR_PATH = '/home/jim/PycharmProjects'\n",
    "import sys\n",
    "sys.path.append(SEMINAR_PATH)\n",
    "\n",
    "import RSreader\n",
    "import RSreader.base.readers as rs\n",
    "from RSreader.base.bbox import BBox\n",
    "from RSreader.io import read_raster, align\n",
    "import RSreader.io as io\n",
    "\n",
    "import seminar\n",
    "from seminar.learn.utils import pred_array, GridSearch, simulated_annealing, contingency_distance, train_predict_conf\n",
    "from seminar.preproc.transformers import FFCombo, TransformerSwitch, InputList, FuncTransformerCombo, DynamicFeatureUnion, FunctionPredictor\n",
    "from seminar.learn.cluster import cluster_based_classif\n",
    "\n",
    "import seminar.preproc.readers as rs\n",
    "from seminar.utils import BiDict, run_jobs\n",
    "from fiona.crs import from_epsg\n",
    "\n",
    "import os\n",
    "import re\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import rasterio as rio\n",
    "from functools import partial\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from collections import OrderedDict\n",
    "import itertools as it\n",
    "\n",
    "from scipy import signal\n",
    "import scipy.fftpack as ff\n",
    "from scipy.fft import fftshift\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import sklearn as skl\n",
    "\n",
    "import sklearn.cluster as cl\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler, FunctionTransformer, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, adjusted_rand_score, adjusted_mutual_info_score, fowlkes_mallows_score\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "import scipy.optimize\n",
    "from scipy.optimize import basinhopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import bokeh.palettes as palettes\n",
    "from bokeh.models import HoverTool\n",
    "\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "import holoviews as hv\n",
    "\n",
    "import dill as pickle\n",
    "from joblib import dump, load\n",
    "import copy\n",
    "\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "gv.extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(seminar.preproc.transformers)\n",
    "reload(seminar.learn.utils)\n",
    "reload(seminar.utils)\n",
    "reload(seminar.learn.cluster)\n",
    "reload(scipy)\n",
    "reload(RSreader.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variable Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs_opt_names = ['blue', 'green', 'red', 're1','re2','re3','nir', 're4', 'swir1', 'swir2']\n",
    "arrs_tex_names = ['vis_rough', 'nir_rough'] + [i + '_' + j for i in ['vis', 'nir'] for j in ['mean', 'var', 'contr', 'hom', 'dis', 'ent', 'm2', 'cor']]\n",
    "indices = ['EVI', 'mNDWI', 'NDBI', 'NDVI', 'NDWI', 'NDSI', 'UI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_names = ['vv', 'vh'] + arrs_opt_names + indices + arrs_tex_names\n",
    "all_names = ['vv', 'vh'] + indices + arrs_tex_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_colors = 100\n",
    "colorset = list(map(str, range(10))) + ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "colormap = ['#' + ''.join(map(str, np.random.choice(colorset, 6))) for i in range(n_colors)]\n",
    "\n",
    "cmap = {i: c for i, c in enumerate(colormap)}\n",
    "cmap[-1] = '#ffffff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = ['#f64e5d', '#fa6d55', '#e7693f', '#8885a9', '#666551', '#f6c55b', \n",
    "            '#212a47', '#ebeed1', '#67d6ca', '#118e55', '#8442e4', '#28ba56', \n",
    "            '#bc0aca', '#63e1ad', '#05b310', '#0fbccd', '#fb48be', '#cf67e1', \n",
    "            '#084c63', '#cee907', '#785044', '#cfde6c', '#9b9d6a', '#9abfb1', \n",
    "            '#abd9f1', '#f38902', '#62e138', '#84b069', '#910c57', '#7289de', \n",
    "            '#4c1072', '#772b44', '#a23b82', '#e61f62', '#97c9a0', '#b33200', \n",
    "            '#c01f7a', '#2c39d5', '#45f65c', '#b0d585', '#1aff26', '#e8eb80', \n",
    "            '#3caa57', '#c2f160', '#1fb237', '#71f62c', '#a034b8', '#114789', \n",
    "            '#6f941e', '#ec9dea', '#902f73', '#e6aa60', '#ad80f8', '#53bace',\n",
    "            '#780dcd', '#5a7c18', '#fd22d7', '#4288e5', '#effb3f', '#82cf91', \n",
    "            '#4e0c43', '#ea4298', '#d80202', '#d5536b', '#c67615', '#321038', \n",
    "            '#5a32ba', '#d7d1f1', '#ec41c3', '#65c7f5', '#1dda6f', '#76682b', \n",
    "            '#c07691', '#0cf12d', '#115b78', '#d1f0ee', '#59b790', '#237111', \n",
    "            '#aae21d', '#e27639', '#035c61', '#3fae55', '#3f4e22', '#c3accc', \n",
    "            '#6788de', '#cf9cd5', '#579ab9', '#313222', '#f6f7c1', '#d88493', \n",
    "            '#57c531', '#d0b987', '#f6f930', '#870496', '#9923a1', '#65312c', \n",
    "            '#aa0312', '#7fb935', '#d68823', '#33455f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_tkwargs_fft = ([{'pre_feature_name': 'vv fr ', 'time_step': 6 / 360}, \n",
    "                        {'pre_feature_name': 'vh fr ', 'time_step': 6 / 360}] \n",
    "                       #+ [{'pre_feature_name': name + ' fr', 'time_step': 10 / 360} for name in arrs_opt_names]\n",
    "                        + [{'pre_feature_name': name + ' fr', 'time_step': 10 / 360} for name in indices] \\\n",
    "                        + [{'pre_feature_name': name + ' fr', 'time_step': None} for name in arrs_tex_names]) # this is just for completeness, doesn't make sense with the given temporal resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = BBox((5.905e5, 4.835e6, 6.6997e5, 4.798e5), crs=from_epsg(32631))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read VV\n",
    "# Read in data, crop first, then load from dir with cropped\n",
    "reader_vv = rs.SeminarReader(time_pattern=r'_([_\\-0-9]+)_',\n",
    "                             incl_pattern='.*',\n",
    "                             #dirpath='/home/jim/PycharmProjects/seminar/notebooks/data/vv_6d_12d_cropped')\n",
    "                             dirpath='/home/jim/mount/shared/course/geo441/stud/B_Camargue/s1_crop/vv_dB')\n",
    "arrs_vv, bboxs_vv = reader_vv.query()\n",
    "arrs_vv.name = 'vv'\n",
    "arrs_vv.data = arrs_vv.data.astype(np.float16)\n",
    "\n",
    "\n",
    "# Read VH\n",
    "# Read in data, crop first, then load from dir with cropped\n",
    "reader_vh = rs.SeminarReader(time_pattern=r'_([_\\-0-9]+)_',\n",
    "                             incl_pattern='.*',\n",
    "                             #dirpath='/home/jim/PycharmProjects/seminar/notebooks/data/vh_6d_12d_cropped')\n",
    "                             dirpath='/home/jim/mount/shared/course/geo441/stud/B_Camargue/s1_crop/vh_dB')\n",
    "arrs_vh, bboxs_vh = reader_vh.query()\n",
    "arrs_vh.name = 'vh'\n",
    "arrs_vh.data = arrs_vh.data.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Optical\n",
    "# Beware, this is much data, make sure your RAM is large and empty, ~ 10 gb\n",
    "\n",
    "reader_opt = rs.SeminarReader(time_pattern=r'_([_\\-0-9]+)_',\n",
    "                             incl_pattern='.*',\n",
    "                             #dirpath='/home/jim/PycharmProjects/seminar/notebooks/data/vh_6d_12d_cropped')\n",
    "                             dirpath='/home/jim/mount/shared/course/geo441/stud/B_Camargue/optical/FR_S2_composite_filt_crop')\n",
    "arrs_opt, bboxs_opt = reader_opt.query(chunks=1000)\n",
    "arrs_opt.name = 'opt'\n",
    "arrs_opt.data = (arrs_opt.data * 10000).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrs_opt = arrs_opt.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Indices\n",
    "# Read in data, crop first, then load from dir with cropped\n",
    "arrs_indices = []\n",
    "readers_ind = []\n",
    "for index in indices:\n",
    "    reader = rs.SeminarReader(time_pattern=r'_([_\\-0-9]+)_',\n",
    "                                 incl_pattern='.*',\n",
    "                                 #dirpath='/home/jim/PycharmProjects/seminar/notebooks/data/vv_6d_12d_cropped')\n",
    "                                 dirpath='/home/jim/mount/shared/course/geo441/stud/B_Camargue/indices/' \n",
    "                                         +'FR_S2_composite_filt_%s_crop' % index)\n",
    "                                  #dirpath='/home/jim/mount/shared/course/geo441/data/2020_Camargue/optical')\n",
    "    arrs_ind, _ = reader.query()#chunks=1000)  # bbox=bbox, out=True, out_dir='/home/jim/PycharmProjects/seminar/notebooks/data/test_out')\n",
    "    arrs_ind.name = index\n",
    "    arrs_ind.data = arrs_ind.data.astype(np.float16)\n",
    "    arrs_indices.append(arrs_ind)\n",
    "    readers_ind.append(reader)\n",
    "    \n",
    "#arrs_indices = xr.concat(arrs_indices, 'band')\n",
    "#arrs_indices.name = 'ind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read textures\n",
    "bnds = ['vis', 'nir']\n",
    "arrs_tex = []\n",
    "readers_tex = []\n",
    "\n",
    "for bnd in bnds:\n",
    "    base_dir = '/home/jim/mount/shared/course/geo441/stud/B_Camargue/optical/FR_S2_textures/%s' % bnd\n",
    "\n",
    "    reader_tex = rs.SeminarReader(time_pattern=r'_(\\d+)_',\n",
    "                             incl_pattern='_roughness',\n",
    "                             #dirpath='/home/jim/PycharmProjects/seminar/notebooks/data/vh_6d_12d_cropped')\n",
    "                             dirpath=base_dir)\n",
    "    arr_tex, _ = reader_tex.query()\n",
    "    arrs_tex.append(arr_tex)\n",
    "    readers_tex.append(reader_tex)\n",
    "\n",
    "for bnd in bnds:\n",
    "    base_dir = '/home/jim/mount/shared/course/geo441/stud/B_Camargue/optical/FR_S2_textures/%s' % bnd\n",
    "\n",
    "    reader_tex = rs.SeminarReader(time_pattern=r'_(\\d+)_',\n",
    "                             incl_pattern='5x5_copy',\n",
    "                             #dirpath='/home/jim/PycharmProjects/seminar/notebooks/data/vh_6d_12d_cropped')\n",
    "                             dirpath=base_dir)\n",
    "    arr_tex, _ = reader_tex.query()\n",
    "    arrs_tex.append(arr_tex)\n",
    "    readers_tex.append(reader_tex)\n",
    "\n",
    "#for bnd in bnds:\n",
    "#    base_dir = '/home/jim/mount/shared/course/geo441/stud/B_Camargue/optical/FR_S2_textures/%s' % bnd\n",
    "#\n",
    "#    reader_tex = rs.SeminarReader(time_pattern=r'_(\\d+)_',\n",
    "#                             incl_pattern='5x5_copy',\n",
    "#                             #dirpath='/home/jim/PycharmProjects/seminar/notebooks/data/vh_6d_12d_cropped')\n",
    "#                             dirpath=base_dir)\n",
    "#    arr_tex, _ = reader_tex.query()\n",
    "#    arrs_tex.append(arr_tex)\n",
    "#    readers_tex.append(reader_tex)\n",
    "    \n",
    "#for bnd in bnds:\n",
    "#    base_dir = '/home/jim/mount/shared/course/geo441/stud/B_Camargue/optical/FR_S2_textures/%s' % bnd\n",
    "#\n",
    "#    reader_tex = rs.SeminarReader(time_pattern=r'_(\\d+)_',\n",
    "#                             incl_pattern='7x7_copy',\n",
    "#                             #dirpath='/home/jim/PycharmProjects/seminar/notebooks/data/vh_6d_12d_cropped')\n",
    "#                             dirpath=base_dir)\n",
    "#    arr_tex, _ = reader_tex.query()\n",
    "#    arrs_tex.append(arr_tex)\n",
    "#    readers_tex.append(reader_tex)\n",
    "    \n",
    "arrs_tex = xr.concat(arrs_tex, 'band')\n",
    "arrs_tex.name = 'tex'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth: Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ground_truth(ground_truth):\n",
    "    # Bring ground truth in canonical form\n",
    "    uniques = np.unique(ground_truth)\n",
    "    mapping = {code: code for val, code in enumerate(uniques)}\n",
    "    mapping[0] = -1\n",
    "    mapping[-1] = -1\n",
    "\n",
    "    mapper = np.vectorize(lambda entry: mapping.get(entry, entry))\n",
    "    ground_truth.data = mapper(ground_truth.data).astype(np.int16)\n",
    "    bimapping = BiDict(mapping)\n",
    "    \n",
    "    return ground_truth, bimapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_class_hierarchy_graph(classes):\n",
    "        graph = nx.Graph()\n",
    "        graph.add_node('0')\n",
    "        recurse_graph_levels('0', graph, 0, classes)\n",
    "        return graph\n",
    "\n",
    "def recurse_graph_levels(parent_node, graph, new_layer, subset_classes):\n",
    "    new_nodes = set([str(cls_out)[new_layer] for cls_out in subset_classes if new_layer < len(str(cls_out))])\n",
    "\n",
    "    if new_nodes != set():\n",
    "        for new_node in new_nodes:\n",
    "            new_subset = filter(lambda x: x.startswith(parent_node[1:] + new_node), subset_classes)\n",
    "            recurse_graph_levels(parent_node + new_node, graph, new_layer + 1, new_subset)\n",
    "            graph.add_node(parent_node + new_node)\n",
    "            graph.add_edge(parent_node, parent_node + new_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ground truth and align\n",
    "ground_truth_classif, bboxs = io.read_raster('/home/jim/Dropbox/RS-Camargue-Box/Klassierung_2/pnrc_mos_2016_c_rasterized30x30m_UTM31N.tif')\n",
    "ground_truth_classif.data = ground_truth_classif.astype(rio.int32)\n",
    "\n",
    "aligned_gt = align(arrs_vv, ground_truth_classif,\n",
    "                   '/home/jim/PycharmProjects/seminar/notebooks/data/aligned_ground_truth.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_gt, bimapping = clean_ground_truth(ground_truth=aligned_gt)\n",
    "gt = pd.DataFrame(aligned_gt.data.flatten()).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gt = np.where(gt > -1)\n",
    "only_valid_gt = gt.iloc[valid_gt[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth: Reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_classif, bboxs = io.read_raster('/home/jim/mount/shared/course/geo441/stud/B_Camargue/alternativ_pnrc_classes/pnrc_2016_DGL_mod.tif')\n",
    "\n",
    "ground_truth_classif.data = ground_truth_classif.astype(rio.int32)\n",
    "aligned_gt_reduced = align(arrs_vv, ground_truth_classif, '/home/jim/PycharmProjects/seminar/notebooks/data/aligned_ground_truth_reduced.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_gt_reduced, bimapping_reduced = clean_ground_truth(ground_truth=aligned_gt_reduced)\n",
    "gt_reduced = pd.DataFrame(aligned_gt_reduced.data.flatten()).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gt = np.where(gt_reduced > -1)\n",
    "only_valid_gt_reduced = gt_reduced.iloc[valid_gt[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_gt.hvplot.image(width=1200, height=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Water masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_wm = rs.SeminarReader(time_pattern=r'_([_\\-0-9]+)_',\n",
    "                             incl_pattern='.*',\n",
    "                             #dirpath='/home/jim/PycharmProjects/seminar/notebooks/data/vh_6d_12d_cropped')\n",
    "                             dirpath='/home/jim/mount/shared/course/geo441/stud/B_Camargue/watermask/S1_vh_watermask/')\n",
    "arrs_wm, bboxs_wm = reader_wm.query(chunks=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_wm_all, _ = io.read_raster('/home/jim/mount/shared/course/geo441/stud/B_Camargue/watermask/S1_vh_watermask_extent/S1IWGRDH_VH_watermask_maxExtent.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_wm_yearly_max = rs.SeminarReader(time_pattern=r'_([_\\-0-9]+)',\n",
    "                             incl_pattern='_maxExtent_',\n",
    "                             #dirpath='/home/jim/PycharmProjects/seminar/notebooks/data/vh_6d_12d_cropped')\n",
    "                             dirpath='/home/jim/mount/shared/course/geo441/stud/B_Camargue/watermask/S1_vh_watermask_extent/yearly/')\n",
    "arrs_wm_yearly_max, bboxs_wm = reader_wm_yearly_max.query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_wm_yearly_min = rs.SeminarReader(time_pattern=r'_([_\\-0-9]+)',\n",
    "                             incl_pattern='_maxExtent_',\n",
    "                             #dirpath='/home/jim/PycharmProjects/seminar/notebooks/data/vh_6d_12d_cropped')\n",
    "                             dirpath='/home/jim/mount/shared/course/geo441/stud/B_Camargue/watermask/S1_vh_watermask_extent/yearly/')\n",
    "arrs_wm_yearly_min, bboxs_wm = reader_wm_yearly_min.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_unnorm = arrs_vv.data.reshape(arrs_vv.shape[0], -1).transpose()\n",
    "only_valid_vv = pd.DataFrame(xx_unnorm, columns=arrs_vv.time.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_unnorm = arrs_vh.data.reshape(arrs_vv.shape[0], -1).transpose()\n",
    "only_valid_vh = pd.DataFrame(xx_unnorm, columns=arrs_vh.time.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INDICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_unnorm = [ind.data.reshape(ind.shape[0], -1).transpose() for ind in arrs_indices]\n",
    "only_valid_ind = [pd.DataFrame(ind, columns=arrs_indices[i].time.data) for i, ind in enumerate(xx_unnorm)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xx_unnorm = [arrs_opt[:, i, ...].data.reshape(arrs_opt.shape[0], -1).transpose() for i in range(arrs_opt.shape[1])]\n",
    "#only_valid_opt = [pd.DataFrame(opt, columns=arrs_opt[:, i].time.data) for i, opt in enumerate(xx_unnorm)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_unnorm = [arrs_tex[:, i, ...].data.reshape(arrs_tex.shape[0], -1).transpose() for i in range(arrs_tex.shape[1])]\n",
    "only_valid_tex = [pd.DataFrame(tex, columns=arrs_tex[:, i].time.data) for i, tex in enumerate(xx_unnorm)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_time(X, year):\n",
    "    if type(X) is InputList:\n",
    "        in_cols = [[i for i, col in enumerate(X.get(name).columns) if col.year == year] for name in X.names]\n",
    "        return InputList(OrderedDict([(X.names[i], x.iloc[:, in_cols[i]]) for i, x in enumerate(X.list)]))\n",
    "    else:\n",
    "        in_cols = [[i for i, col in enumerate(X[name].columns) if col.year == year] for name in X.keys()]\n",
    "        return OrderedDict([(key, x.iloc[:, in_cols[i]]) for i, (key, x) in enumerate(X.items())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_dframe_with_dframe(arr, mask):\n",
    "    if type(arr) is InputList:\n",
    "        inds = mask.iloc[np.where(mask == 0)[0]].index\n",
    "        return InputList(OrderedDict([(arr.names[key], val.drop(inds, errors='ignore')) for key, val in arr.map.items() if val is not None]))\n",
    "    else:\n",
    "        return arr.drop(mask.iloc[np.where(mask == 0)[0]].index, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from dataset\n",
    "def sample(n, ground_truth, mask=None):\n",
    "    if type(ground_truth) is list:\n",
    "        if mask is not None:\n",
    "            ground_truth = [mask_dframe_with_dframe(gt, mask) for gt in ground_truth]\n",
    "        intersect = ground_truth[0].index.intersection(ground_truth[1].index)\n",
    "        for i in range(2, len(ground_truth)):\n",
    "            intersect = intersect.intersection(ground_truth[i].index)\n",
    "        ground_truth = ground_truth[0].loc[intersect]\n",
    "    else:\n",
    "        if mask is not None:\n",
    "            ground_truth = mask_dframe_with_dframe(ground_truth, mask)\n",
    "\n",
    "    sample_inds = ground_truth.sample(n=n)\n",
    "    samples = OrderedDict([(name, df.loc[sample_inds.index, :]) for name, df in only_valid_dfxx_unnorm.items()])\n",
    "    \n",
    "    gt_samples = ground_truth.loc[sample_inds.index]\n",
    "    return samples, gt_samples, sample_inds.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_gt(gt, depth_wetland, depth_nonwetland):\n",
    "    mapper = np.vectorize(lambda entry, dw, dnw: str(bimapping.inverse.get(entry)[-1])[:dw ]  if entry >= 44  else str(bimapping.inverse.get(entry)[-1])[:dnw ] )\n",
    "    gtt = mapper(gt, depth_wetland, depth_nonwetland).astype(np.int16)\n",
    "    \n",
    "    # make invalids take same number\n",
    "    gtt[np.where(gtt == 0)] = -1\n",
    "    \n",
    "    return gtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dfxx_unnorm = OrderedDict([('vv', only_valid_vv), ('vh', only_valid_vh)])\n",
    "full_dfxx_unnorm.update(OrderedDict([(name, ind) for name, ind in zip(indices, only_valid_ind)]))\n",
    "full_dfxx_unnorm.update(OrderedDict([(name, tex) for name, tex in zip(arrs_tex_names, only_valid_tex)]))\n",
    "\n",
    "only_valid_dfxx_unnorm = full_dfxx_unnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, gt_samples, sample_index = sample(1000, ground_truth=only_valid_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define different Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(pixs, with_cloud_mask=False):\n",
    "    ret = OrderedDict()\n",
    "\n",
    "    # s1 bands\n",
    "    rang = np.array([-20, 5])\n",
    "    for name, pix in pixs.get('vv', 'vh', outtyp='dict').items(): #pixs.get(slice(None, 2)):\n",
    "        if pix is not None:\n",
    "            ret_pix = pix.copy()\n",
    "            ret_pix = np.clip((ret_pix - rang.mean()) / (rang[1] - rang[0]), -1, 1)\n",
    "            ret[name] = ret_pix\n",
    "        \n",
    "    # opt bands\n",
    "    rang = np.array([0, 65520])\n",
    "    for name, pix in pixs.get(*arrs_opt_names, outtyp='dict').items(): #pixs.get(slice(2, 2+len(arrs_opt_names))):\n",
    "        if pix is not None:\n",
    "            ret_pix = pix.copy().astype(np.float32)\n",
    "            ret_pix = (ret_pix - rang.mean()).astype(np.float32) / (rang[1] - rang[0])\n",
    "            ret[name] = ret_pix\n",
    "        \n",
    "    # index bands\n",
    "    rang = np.array([-10000, 10000])\n",
    "    for name, pix in pixs.get(*indices, outtyp='dict').items(): #pixs.get(slice(2+len(arrs_opt_names), None)):\n",
    "        if pix is not None:\n",
    "            ret_pix = pix.copy()\n",
    "            ret_pix = np.clip((ret_pix - rang.mean()) / (rang[1] - rang[0]), -1, 1)\n",
    "            ret[name] = ret_pix\n",
    "        \n",
    "    # tex bands\n",
    "    rang = np.array([-100, 100])\n",
    "    for name, pix in pixs.get(*arrs_tex_names, outtyp='dict').items(): #pixs.get(slice(2+len(arrs_opt_names), None)):\n",
    "        if pix is not None:\n",
    "            ret_pix = pix.copy()\n",
    "            ret_pix = np.clip((ret_pix - rang.mean()) / (rang[1] - rang[0]), -1, 1)\n",
    "            ret[name] = ret_pix\n",
    "    \n",
    "    return InputList(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normer = FunctionTransformer(norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_norm = FunctionTransformer(func=lambda X: np.einsum('ij, i -> ij', X - X.mean(axis=1)[:, None], \n",
    "                                                        1 / (X.std(axis=1) + 1e-2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(X):\n",
    "    if hasattr(X, 'values'):\n",
    "        X = X.values\n",
    "    return X.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_func_transformer(func, *fargs, **fkwargs):\n",
    "    pip = Pipeline([#('scale', std_norm),#RobustScaler(quantile_range=quantile_range)),\n",
    "                    #('stop_inf', stop_inf), \n",
    "                    ('func', FunctionTransformer(lambda X: func(X, *fargs, **fkwargs))),\n",
    "                    ('reshape', FunctionTransformer(reshape)),\n",
    "                    #('scale', RobustScaler(quantile_range=quantile_range))])\n",
    "                    ('scale_out', StandardScaler()), \n",
    "                    ('clip', FunctionTransformer(lambda x: np.clip(x, -3, 3))),])\n",
    "    return pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_suwi(X, reducer=np.mean, *args, **kwargs):\n",
    "    su_cols = [i for i, c in enumerate(X.columns) if c.month >= 6 and  c.month <= 9]\n",
    "    wi_cols = [i for i, c in enumerate(X.columns) if c.month <= 2 or  c.month >= 11]\n",
    "    \n",
    "    if len(su_cols) > 0 and len(wi_cols) > 0:\n",
    "        return reducer(X.iloc[:, su_cols], axis=1) - reducer(X.iloc[:, wi_cols], axis=1)\n",
    "    else:\n",
    "        return np.zeros((X.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Plain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_concerned=[True, False] + [False, False, False] + [False] * (len(arrs_opt_names) - 3) + [False] * len(indices)\n",
    "is_concerned=[True, False] + [False] * len(indices) + [False] * len(arrs_tex_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_plain = Pipeline([('scale', normer),\n",
    "                      ('preproc', FFCombo(is_concerned=is_concerned, len_inp=len(is_concerned),\n",
    "                                          scaler=std_norm, thr_freq=5, scale=True, quantile_range=(0.25, 0.75), \n",
    "                                          ordered_tkwargs=ordered_tkwargs_fft)),\n",
    "                      ('clustering', cl.KMeans(n_clusters=8, n_jobs=12))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = pip_plain.fit(InputList(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = pip_plain['clustering'].cluster_centers_\n",
    "inds = pip_plain['preproc'].get_feature_names()\n",
    "pd.DataFrame(a.transpose(), index=inds).hvplot(rot=90, width=1200, height=400, cmap=colormap[:pip_plain['clustering'].n_clusters]) #, xformatter='%.3f')#.opts(colorbar=True, cmap='paired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr, pred = pred_array(model=pip_plain, inp=InputList(only_valid_dfxx_unnorm), model_arr=arrs_vv[0, 0], n_batches=400, n_jobs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arr.hvplot.image('x', 'y', width=900, height=900, rasterize=False).opts(colorbar=True, cmap=['#ffffff'] + colormap[:pip_plain['clustering'].n_clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = [np.average(only_valid_dfxx_unnorm.loc[pred.index[np.where(pred == cls_ind)[0]], :], axis=0) \n",
    "     for cls_ind in range(pip['clustering'].n_clusters)]\n",
    "means = np.stack(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(means.transpose()).hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, gt_samples, sample_index = sample(10000, ground_truth=only_valid_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_concerned = [False, True] + [False] * len(arrs_opt_names) + [False] * len(indices)\n",
    "#is_concerned_ext = [True, True] + [True, True, True] + [False]* (len(arrs_opt_names) - 3) + [True] * len(indices)\n",
    "\n",
    "is_concerned = [False, True] + [False] * len(indices) + [False] * len(arrs_tex_names)\n",
    "is_concerned_ext = [True, True] + [True] * len(indices) + [True] * len(arrs_tex_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_fft = Pipeline([('preproc', FFCombo(len_inp=len(samples), is_concerned=is_concerned,\n",
    "                                              scaler=std_norm, thr_freq=5, scale=True, quantile_range=(0.25, 0.75), \n",
    "                                              ordered_tkwargs=ordered_tkwargs_fft))\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_other = Pipeline([('preproc', \n",
    "                       FeatureUnion([\n",
    "                           #('mean', FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                           #                                       transformer=get_scaled_func_transformer(np.mean, axis=1))),\n",
    "                           ('diff_suwi_mean', FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                  transformer=get_scaled_func_transformer(diff_suwi))),]))\n",
    "                           #('std', FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), #edge_cols=edges,\n",
    "                           #                                   transformer=get_scaled_func_transformer(np.en, axis=1))),\n",
    "                           #('max', FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples),\n",
    "                           #                             transformer=get_scaled_func_transformer(np.max, axis=1)))]))\n",
    "                     ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Pipeline([('scale', normer),\n",
    "                    ('feats', FeatureUnion([('scaled_fft', pip_fft),\n",
    "                                            ('scaled_ext', pip_other)]))\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_ext = Pipeline([('preproc', preproc),\n",
    "                    #('clustering', cl.KMeans(n_clusters=10, n_jobs=12)),\n",
    "                    ('clustering', TransformerSwitch(transformers=[cl.KMeans(n_clusters=100, n_jobs=12),\n",
    "                                                                   cl.AffinityPropagation(max_iter=500),\n",
    "                                                                   cl.AgglomerativeClustering(n_clusters=10, linkage='ward'),\n",
    "                                                                   GaussianMixture(n_components=10)\n",
    "                                                                  ], is_on=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = pip_ext.fit(InputList(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#a = pip_ext['clustering'].get().cluster_centers_\n",
    "a = pip_ext['clustering'].get().means_\n",
    "\n",
    "inds = (pip_fft['preproc'].get_feature_names() + \\\n",
    "              #['mean %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i]] + \\\n",
    "              ['diff_suwi %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i]])# + \\\n",
    "              #['max %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i]])\n",
    "pd.DataFrame(a.transpose(), index=inds).hvplot(rot=90, width=1200, height=400, cmap=colormap) #cmap=['#ffffff'] + palettes.Category10[8]) #, xformatter='%.3f')#.opts(colorbar=True, cmap='paired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = mask_dframe_with_dframe(InputList(only_valid_dfxx_unnorm), pd.DataFrame(arr_wm_all.data.flatten()))\n",
    "arr, pred = pred_array(model=pip_ext, inp=inp, model_arr=arrs_vv[0, 0], n_batches=500, n_jobs=5, verbose=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arr.hvplot.image('x', 'y', width=900, height=900, \n",
    "                               rasterize=False).opts(colorbar=True, cmap=colormap[:10]) #palettes.Category10[8]) #.opts(color_levels=9, colorbar=True, cmap='paired') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = [np.average(only_valid_dfxx_unnorm.loc[pred.index[np.where(pred == cls_ind)[0]], :], axis=0) \n",
    "     for cls_ind in range(pip['clustering'].n_clusters)]\n",
    "means = np.stack(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_valid_dfxx_unnorm.values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(means.transpose()).hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_depth_non_wetland = 2\n",
    "ground_truth_depth_wetland = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, gt_samples, sample_index = sample(50000, ground_truth=only_valid_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, gt_samples, sample_index = sample(50000, ground_truth=only_valid_gt, mask=pd.DataFrame(arr_wm_all.data.flatten()))\n",
    "samples = filter_time(samples, year=2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtt_samples = reduce_gt(gt_samples, ground_truth_depth_wetland, ground_truth_depth_non_wetland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr = reduce_gt(aligned_gt.data, ground_truth_depth_wetland, ground_truth_depth_non_wetland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr = xr.DataArray(gtr, dims=['band', 'y', 'x'])\n",
    "gtr.attrs = aligned_gt.attrs\n",
    "gtr = gtr.assign_coords(aligned_gt.coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_concerned = [False, True] + [False] * len(arrs_opt_names) + [False] * len(indices)\n",
    "#is_concerned_ext = [True, True] + [True, True, True] + [False]* (len(arrs_opt_names) - 3) + [True] * len(indices)\n",
    "is_conc_fft_vv_vh = [True, True] + [False] * len(indices) + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_vv_vh_evi = [True, True] + [True, False, False, False, False, False, False] + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_vh = [False, True] + [False] * len(indices) + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_vv = [True, False] + [False] * len(indices) + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_evi = [False, False, True] + [False] * (len(indices) - 1) + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_none = [False, False, False] + [False] * (len(indices) - 1) + [False] * len(arrs_tex_names)\n",
    "\n",
    "is_concerned = is_conc_fft_vv_vh_evi\n",
    "is_concerned_ext = [True, True] + [True] * len(indices) + [True] * len(arrs_tex_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_fft = Pipeline([('fft', FFCombo(len_inp=len(samples), is_concerned=is_concerned,\n",
    "                                              scaler=std_norm, thr_freq=5, scale=True, quantile_range=(0.25, 0.75), \n",
    "                                              ordered_tkwargs=ordered_tkwargs_fft))\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_other = Pipeline([('preproc', \n",
    "                       DynamicFeatureUnion([\n",
    "                           ('min', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.min, axis=1))], is_on=1)),\n",
    "                            \n",
    "                           ('max', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.max, axis=1))], is_on=1)),\n",
    "                           \n",
    "                           ('mean', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.median, axis=1))], is_on=1)),\n",
    "                           ('q25', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(partial(np.quantile, q=0.25), axis=1))], is_on=1)),\n",
    "                           ('q75', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(partial(np.quantile, q=0.75), axis=1))], is_on=1)),\n",
    "                           ('std', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.std, axis=1))], is_on=1)),\n",
    "                           ('median', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.mean, axis=1))], is_on=1)),\n",
    "                           ('diff_suwi_mean', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(diff_suwi, reducer=np.mean))], is_on=1)),\n",
    "                           ('diff_suwi_std', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(diff_suwi, reducer=np.std))], is_on=1)),\n",
    "                           ('diff_suwi_q75', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(diff_suwi, reducer=partial(np.quantile, q=0.75)))], is_on=1)),\n",
    "                           ('diff_suwi_q25', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(diff_suwi, reducer=partial(np.quantile, q=0.25)))], is_on=1))\n",
    "                     ]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Pipeline([#('input_format', FunctionTransformer(to_input_list)), \n",
    "                    ('scale', normer),\n",
    "                    #('pca', TransformerSwitch(transformers=[None,\n",
    "                    #                                        PCA(n_components=40)], is_on=0)),\n",
    "                    #('feats', FeatureUnion([('scaled_fft', pip_fft),\n",
    "                    #                       ('scaled_ext', pip_other)]))\n",
    "                    ('feats', pip_other)\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_ext = Pipeline([('preproc', preproc),\n",
    "                    ('pca', TransformerSwitch(transformers=[None, PCA(n_components=10)], is_on=False, transparent=True)), \n",
    "                    ('clustering', TransformerSwitch(transformers=[cl.KMeans(n_clusters=30, n_jobs=12),\n",
    "                                                                   #cl.AffinityPropagation(max_iter=500),\n",
    "                                                                   GaussianMixture(n_components=150,),\n",
    "                                                                   #cl.AgglomerativeClustering(n_clusters=10, linkage='ward'),\n",
    "                                                                   #cl.MeanShift(max_iter=500, n_jobs=12)\n",
    "                                                                  ], is_on=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = pip_ext.fit(InputList(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pip_ext['clustering'].get().cluster_centers_\n",
    "#a = pip_ext['clustering'].get().means_\n",
    "\n",
    "inds = (#pip_fft['fft'].get_feature_names() + \\\n",
    "              ['mean %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['mean'] is not None] + \n",
    "              ['min %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['min'] is not None] + \n",
    "              ['max %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['max'] is not None] + \n",
    "              ['median %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i]] + \\\n",
    "              ['q25 %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['q25'] is not None] + \n",
    "              ['q75 %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['q75'] is not None] + \n",
    "              ['std %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['std'] is not None] +\n",
    "              ['diff_suwi_mean %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['diff_suwi_mean'] is not None] +\n",
    "              ['diff_suwi_std %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['diff_suwi_std'] is not None] +\n",
    "              ['diff_suwi_q75 %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['diff_suwi_q75'] is not None] +\n",
    "              ['diff_suwi_q25 %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['diff_suwi_q25'] is not None]\n",
    "       )\n",
    "pd.DataFrame(a.transpose(), index=inds).hvplot(rot=90, width=1700, height=700, cmap=colormap) #cmap=['#ffffff'] + palettes.Category10[8]) #, xformatter='%.3f')#.opts(colorbar=True, cmap='paired')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip = pip_ext\n",
    "clf, (valids, conf) = train_predict_conf(model=pip, \n",
    "                                          #dset=InputList(only_valid_dfxx_unnorm), \n",
    "                                          dset=mask_dframe_with_dframe(InputList(only_valid_dfxx_unnorm), pd.DataFrame(arr_wm_all.data.flatten())),\n",
    "                                          model_arr=arrs_vv[0, :1], \n",
    "                                          ground_truth=gtr,\n",
    "                                          classif_out_path='/home/jim/PycharmProjects/seminar/notebooks/data/clustering.tif',\n",
    "                                          samples=InputList(samples),\n",
    "                                          is_aligned=True,\n",
    "                                          load_classif=False,\n",
    "                                          gt=None,\n",
    "                                          train=False,\n",
    "                                          gt_nan=-1,\n",
    "                                          pred_nan=-1, \n",
    "                                          n_jobs=5, chunks=1, verbose=10, n_batches=300, joblib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_labels = np.unique(gtr.data[valids])\n",
    "cluster_labels = np.unique(clf.data[valids])\n",
    "labels = np.r_[cluster_labels, gtr_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_clusters = pip_ext['clustering'].get().n_components #len(pip_ext['clustering'].get().cluster_centers_)\n",
    "n_clusters = pip_ext['clustering'].get().n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(gtr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.hvplot.image('x', 'y', width=1200, height=1200, \n",
    "                               rasterize=False).opts(colorbar=True, cmap=colormap[:n_clusters])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: maximize recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hungarian Method (one-to-one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax3 = plt.subplot(1, 2, 1)\n",
    "ax3.matshow(np.log(conf[n_clusters:, :n_clusters] + 1))\n",
    "ax3.set_title('Unordered Clustering', pad=30)\n",
    "\n",
    "# sort with Hungarian method (*note* this is a bijective mapping BAD!!)\n",
    "row_ind, col_ind = linear_sum_assignment(-conf)\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.matshow(np.log(conf + 1)[row_ind, :][:, col_ind][n_clusters:, n_clusters:])\n",
    "ax2.set_title('Best Bijective Map', pad=25)\n",
    "\n",
    "\n",
    "plt.gcf().set_size_inches((30,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hungarian_assignment = {cluster: labels[col_ind[cluster]] for cluster in np.where(col_ind >= n_clusters)[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hungarian_assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clf = clf.copy()\n",
    "\n",
    "uniques = np.unique(list(hungarian_assignment.values()))\n",
    "idx_map = dict([(j, i) for i, j in enumerate(uniques)])\n",
    "\n",
    "mapper = np.vectorize(lambda cluster: idx_map.get(hungarian_assignment.get(cluster, -1), -1))\n",
    "\n",
    "sorted_clf.data = mapper(clf.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clf.hvplot.image('x', 'y', rasterize=False, height=1200, width=1200, cmap=['#ffffff'] + colormap[:n_classes])# cmap='viridis', # cmap=['#ffffff'] + palettes.Category10[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Assignment (max mean precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set, label_counts = np.unique(gtr.data, return_counts=True)\n",
    "\n",
    "label_to_idx = dict(zip(label_set, range(len(label_set))))\n",
    "label_to_idx[-1] = -1\n",
    "mapper_gtr = np.vectorize(lambda ind: label_to_idx.get(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sorting (maximizing recall)\n",
    "max_assignment = labels[np.argmax(conf, axis=0)][:n_clusters]\n",
    "max_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clf = clf.copy()\n",
    "uniques = np.unique(max_assignment)\n",
    "idx_map = dict([(j, i) for i, j in enumerate(uniques)])\n",
    "\n",
    "mapper = np.vectorize(lambda cluster: label_to_idx.get(max_assignment[cluster], -1))\n",
    "sorted_clf.data = mapper(sorted_clf.data)\n",
    "\n",
    "sorted_gtr = gtr.copy()\n",
    "sorted_gtr.data = mapper_gtr(sorted_gtr.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_clf.hvplot.image('x', 'y', rasterize=False, height=1200, width=1200, \n",
    "                        cmap=colormap[:len(set(sorted_clf.data.flatten())) + 1])# cmap='viridis', # cmap=['#ffffff'] + palettes.Category10[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Procedure: minimize contingency metric over cluster-label assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "label_set, label_counts = np.unique(gtr.data, return_counts=True)\n",
    "\n",
    "# take out -1\n",
    "valid_idx = np.where(label_set != -1)\n",
    "label_set = label_set[valid_idx]\n",
    "label_counts = label_counts[valid_idx]\n",
    "\n",
    "argsort = np.argsort(label_set)\n",
    "label_set = label_set[argsort]\n",
    "label_counts = label_counts[argsort]\n",
    "\n",
    "label_weights = label_counts / np.sum(label_counts)\n",
    "print(dict(zip(label_set, label_weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inds = np.random.choice(range(len(valids[0])), 10000)\n",
    "#inds = list(zip(*[(valids[0][i], valids[1][i], valids[2][i]) for i in inds]))\n",
    "\n",
    "calc_contingency_dist = partial(contingency_distance, score=partial(f1_score, average=None), \n",
    "                                ground_truth=gtr.data.flatten(), clustering=clf.data.flatten(), trn_inds=sample_index) # valid_pix=valids, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0 = max_assignment[:n_clusters].copy()\n",
    "assign, hist, T_end, samples = simulated_annealing(x0, calc_contingency_dist, \n",
    "                                   label_set=label_set, label_weights=label_weights, epochs=100, T=5e-3, eta=0.96, max_change=1, verbose=True, sample=10, subset=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot([h[1].mean() for h in hist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab, res = contingency_distance(prev_perm=x0, score=partial(f1_score, average=None), ground_truth=gtr.data, valid_pix=valids, clustering=clf.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_200_full = assign.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_200_full = {l: 1 - r for l, r in zip(lab, res)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res_200_full, index=[0]).hvplot.bar(xlabel='class', ylabel='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignment_to_label = dict(zip(range(len(assign)), assign))\n",
    "assignment_to_label[-1] = -1  \n",
    "\n",
    "mapper = np.vectorize(lambda ind: label_to_idx.get(assignment_to_label.get(ind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clf = clf.copy()\n",
    "sorted_clf.data = mapper(clf.data)\n",
    "\n",
    "sorted_gtr = gtr.copy()\n",
    "sorted_gtr.data = mapper_gtr(sorted_gtr.data)\n",
    "\n",
    "sorted_dset = xr.Dataset({'gtr':sorted_gtr, 'clf':sorted_clf})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = hv.Image(sorted_clf, kdims=['x', 'y'],).opts(height=900, width=900, cmap=colormap, colorbar=True) + hv.Image(sorted_gtr, kdims=['x', 'y'],).opts(height=900, width=900, cmap=colormap, colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_dset.hvplot.image('x', 'y', ['clf', 'gtr'], height=1200, width=1200, cmap=colormap, colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show new Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reordered(assignment, n_clusters):\n",
    "    assign_argsort = np.argsort(assignment[:n_clusters])\n",
    "    x_ticks = np.where(np.diff(assignment[assign_argsort]) > 0)[0]\n",
    "    x_ticks_labels = assignment[assign_argsort][x_ticks]\n",
    "    return (x_ticks, x_ticks_labels), assign_argsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import colors\n",
    "\n",
    "%matplotlib inline\n",
    "def plot_conf(conf, n_clusters, n_classes, labels):\n",
    "    n_classes = len(np.unique(gtr))\n",
    "\n",
    "    (xticks_assign, xticks_labels_assign), assign_argsort = get_reordered(assign, n_clusters)\n",
    "    conf_reordered_assign = conf[n_clusters:, assign_argsort]\n",
    "\n",
    "    #(xticks_maxassign, xticks_labels_maxassign), maxassign_argsort = get_reordered(max_assignment, n_clusters)\n",
    "    #conf_reordered_maxassign = conf [n_clusters:, maxassign_argsort]\n",
    "\n",
    "\n",
    "    ax2 = plt.subplot(3, 1, 1)\n",
    "    im = ax2.matshow(conf[n_clusters:, :n_clusters], norm=colors.SymLogNorm(vmin=0, vmax=np.max(conf), linthresh=10, base=10), )\n",
    "    ax2.set_title('Unordered Confusion Matrix', pad=30)\n",
    "\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    h = plt.gcf().colorbar(im, cax=cax, orientation='vertical')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax1 = plt.subplot(3, 1, 2)\n",
    "    ax1.matshow(conf_reordered_assign, norm=colors.SymLogNorm(vmin=0, vmax=np.max(conf), linthresh=10, base=10), )\n",
    "    \n",
    "    ax1.set_xticks(ticks=xticks_assign + 0.5)\n",
    "    ax1.set_xticklabels(labels=xticks_labels_assign, rotation=-90)\n",
    "    \n",
    "    ax1.set_yticks(ticks=np.arange(n_classes) + 0.5, )\n",
    "    ax1.set_yticklabels(labels=labels)\n",
    "\n",
    "    plt.vlines(xticks_assign + 0.5, -0.5, conf.shape[0] - n_clusters - 0.5, 'red')\n",
    "    ax1.set_title('Locally Optimal Cluster-Class Map', pad=30)\n",
    "\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    plt.gcf().colorbar(im, cax=cax, orientation='vertical')\n",
    "\n",
    "\n",
    "    plt.gcf().set_size_inches((25,10))\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \"\"\"\n",
    "    ax3 = plt.subplot(3, 1, 2)\n",
    "    ax3.matshow(conf_reordered_maxassign, norm=colors.SymLogNorm(vmin=0, vmax=np.max(conf), linthresh=10), )\n",
    "\n",
    "    plt.xticks(ticks=xticks_maxassign + 0.5, labels=xticks_labels_maxassign, rotation=-90)\n",
    "    plt.yticks(ticks=np.arange(n_classes) + 0.5, labels=labels[n_clusters:])\n",
    "\n",
    "    plt.vlines(xticks_maxassign + 0.5, -0.5, conf.shape[0] - n_clusters - 0.5, 'red')\n",
    "    ax3.set_title('Initial Cluster-Class Map', pad=30)\n",
    "\n",
    "    divider = make_axes_locatable(ax3)\n",
    "    cax = divider.append_axes('right', size='1%', pad=0.05)\n",
    "    plt.gcf().colorbar(im, cax=cax, orientation='vertical')\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_valid_gt.index.intersection(only_valid_gt_reduced.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, gt_samples, sample_index = sample(100000, ground_truth=[only_valid_gt, only_valid_gt_reduced], mask=pd.DataFrame(arr_wm_all.data.flatten()))\n",
    "gt_simple = only_valid_gt_reduced.loc[sample_index]\n",
    "\n",
    "samples  = filter_time(samples, year=year)\n",
    "\n",
    "gt_red = reduce_gt(gt_samples, depth_nonwetland=2, depth_wetland=2)\n",
    "gt_red1 = reduce_gt(gt_samples, depth_nonwetland=2, depth_wetland=3)\n",
    "gt_red2 = reduce_gt(gt_samples, depth_nonwetland=2, depth_wetland=4)\n",
    "gt_red3 = reduce_gt(gt_samples, depth_nonwetland=3, depth_wetland=3)\n",
    "gt_red4 = reduce_gt(gt_samples, depth_nonwetland=3, depth_wetland=4)\n",
    "gt_red5 = reduce_gt(gt_samples, depth_nonwetland=4, depth_wetland=4)\n",
    "gts = [gt_simple.values, gt_red, gt_red1, gt_red2, gt_red3, gt_red4, gt_red5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_concerned = [False, True] + [False] * len(arrs_opt_names) + [False] * len(indices)\n",
    "#is_concerned_ext = [True, True] + [True, True, True] + [False]* (len(arrs_opt_names) - 3) + [True] * len(indices)\n",
    "is_conc_fft_vv_vh = [True, True] + [False] * len(indices) + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_vv_vh_evi = [True, True] + [True, False, False, False, False, False, False] + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_vh = [False, True] + [False] * len(indices) + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_vv = [True, False] + [False] * len(indices) + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_evi = [False, False, True] + [False] * (len(indices) - 1) + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_none = [False, False, False] + [False] * (len(indices) - 1) + [False] * len(arrs_tex_names)\n",
    "\n",
    "is_concerned = is_conc_fft_vv_vh_evi\n",
    "is_concerned_ext = [True, True] + [True] * len(indices) + [True] * len(arrs_tex_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_fft = Pipeline([('fft', FFCombo(len_inp=len(samples), is_concerned=is_concerned,\n",
    "                                              scaler=std_norm, thr_freq=5, scale=True, quantile_range=(0.25, 0.75), \n",
    "                                              ordered_tkwargs=ordered_tkwargs_fft))\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_other = Pipeline([('preproc', \n",
    "                       DynamicFeatureUnion([\n",
    "                           ('min', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.min, axis=1))], is_on=1)),\n",
    "                            \n",
    "                           ('max', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.max, axis=1))], is_on=1)),\n",
    "                           \n",
    "                           ('mean', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.median, axis=1))], is_on=1)),\n",
    "                           ('q25', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(partial(np.quantile, q=0.25), axis=1))], is_on=1)),\n",
    "                           ('q75', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(partial(np.quantile, q=0.75), axis=1))], is_on=1)),\n",
    "                           ('std', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.std, axis=1))], is_on=1)),\n",
    "                           ('median', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.mean, axis=1))], is_on=1)),\n",
    "                           ('diff_suwi_mean', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(diff_suwi, reducer=np.mean))], is_on=0)),\n",
    "                           ('diff_suwi_std', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(diff_suwi, reducer=np.std))], is_on=1)),\n",
    "                           ('diff_suwi_q75', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(diff_suwi, reducer=partial(np.quantile, q=0.75)))], is_on=1)),\n",
    "                           ('diff_suwi_q25', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=len(samples), \n",
    "                                                                                        transformer=get_scaled_func_transformer(diff_suwi, reducer=partial(np.quantile, q=0.25)))], is_on=1))\n",
    "                     ]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Pipeline([#('input_format', FunctionTransformer(to_input_list)), \n",
    "                    ('scale', normer),\n",
    "                    #('pca', TransformerSwitch(transformers=[None,\n",
    "                    #                                        PCA(n_components=40)], is_on=0)),\n",
    "                    ('feats', DynamicFeatureUnion([('scaled_fft', pip_fft),\n",
    "                                            ('scaled_ext', pip_other)]))\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_ext = Pipeline([('preproc', preproc),\n",
    "                    ('pca', TransformerSwitch(transformers=[None, PCA(n_components=10)], is_on=False, transparent=True)), \n",
    "                    ('clustering', TransformerSwitch(transformers=[cl.KMeans(n_clusters=10, n_jobs=12),\n",
    "                                                                   #cl.AffinityPropagation(max_iter=500),\n",
    "                                                                   GaussianMixture(n_components=10,),\n",
    "                                                                   #cl.AgglomerativeClustering(n_clusters=10, linkage='ward'),\n",
    "                                                                   #cl.MeanShift(max_iter=500, n_jobs=12)\n",
    "                                                                  ], is_on=0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = pip_ext.fit(InputList(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = pip_ext['clustering'].get().cluster_centers_\n",
    "#a = pip_ext['clustering'].get().means_\n",
    "\n",
    "inds = (pip_fft['fft'].get_feature_names() + \\\n",
    "              ['mean %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['mean'] is not None] + \n",
    "              ['min %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['min'] is not None] + \n",
    "              ['max %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['max'] is not None] + \n",
    "              #['median %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i]] + \\\n",
    "              ['q25 %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['q25'] is not None] + \n",
    "              ['q75 %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['q75'] is not None] + \n",
    "              ['std %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['std'] is not None] +\n",
    "              ['diff_suwi_mean %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['diff_suwi_mean'] is not None] +\n",
    "              ['diff_suwi_std %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['diff_suwi_std'] is not None] +\n",
    "              ['diff_suwi_q75 %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['diff_suwi_q75'] is not None] +\n",
    "              ['diff_suwi_q25 %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['diff_suwi_q25'] is not None]\n",
    "       )\n",
    "pd.DataFrame(a.transpose(), index=inds).hvplot(rot=90, width=1700, height=400, cmap=colormap) #cmap=['#ffffff'] + palettes.Category10[8]) #, xformatter='%.3f')#.opts(colorbar=True, cmap='paired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'preproc__feats__scaled_ext__preproc__median__is_on': [1],\n",
    "              'preproc__feats__scaled_ext__preproc__q25__is_on': [1],\n",
    "              'preproc__feats__scaled_ext__preproc__q75__is_on': [1],\n",
    "              'preproc__feats__scaled_ext__preproc__min__is_on': [1],\n",
    "              'preproc__feats__scaled_ext__preproc__max__is_on': [1],\n",
    "              'preproc__feats__scaled_ext__preproc__std__is_on': [1],\n",
    "              'preproc__feats__scaled_ext__preproc__mean__is_on': [1],\n",
    "              'preproc__feats__scaled_ext__preproc__diff_suwi_mean__is_on': [1],\n",
    "              'preproc__feats__scaled_ext__preproc__diff_suwi_std__is_on': [0],\n",
    "              'preproc__feats__scaled_ext__preproc__diff_suwi_q25__is_on': [1],\n",
    "              'preproc__feats__scaled_ext__preproc__diff_suwi_q75__is_on': [1],\n",
    "              #'preproc__feats__scaled_fft__fft__thr_freq': [3, 4, 5, 6, 10],\n",
    "              'preproc__feats__scaled_fft__fft__is_concerned': [is_conc_fft_vv_vh_evi],#is_conc_fft_vv_vh, ],\n",
    "              #'pca__is_on': [False, True],\n",
    "              #'pca__n_components': [10, 20, 40, 60],\n",
    "              #'clustering__is_on': [0],\n",
    "              'clustering__n_clusters': [10, 20, 30, 40, 50, 60, 100, 200]}\n",
    "              #'clustering__n_components': ['--', 30, 50, 70, 100, 150, 200],}\n",
    "              #'clustering__linkage': ['--', 'single', 'average', 'complete']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearch(pip_ext, param_grid, cv=4, scores=['adjusted_rand_score', 'adjusted_mutual_info_score', 'fowlkes_mallows_score', 'v_measure_score'])\n",
    "res, params = gs.fit(InputList(samples), gts, joblib=True, n_jobs=8, verbose=50)\n",
    "res, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=list(param_grid.keys()) + list(gs.scores) + ['gt'])\n",
    "for i in range(len(res)):\n",
    "    for j in range(len(res[i])):\n",
    "        #params[j]['preproc__feats__scaled_fft__fft__is_concerned'] = np.sum(params[j]['preproc__feats__scaled_fft__fft__is_concerned'])\n",
    "        row = dict(**dict(zip(gs.scores, res[i][j]), **params[j]))\n",
    "        row['gt'] = i\n",
    "        #row['preproc__feats__scaled_fft__fft__is_concerned'] = 0#np.sum(row['preproc__feats__scaled_fft__fft__is_concerned'] > 0)\n",
    "        df = df.append(row, ignore_index=True)\n",
    "        \n",
    "filter_col = [col for col in df if col.startswith('preproc__feats__scaled')]\n",
    "df[filter_col] = df[filter_col].replace('--', 0)\n",
    "\n",
    "#df['clustering__n_clusters'] = df['clustering__n_clusters'].replace('--', 10)\n",
    "\n",
    "uniques = np.unique(df['preproc__feats__scaled_fft__fft__is_concerned'])\n",
    "df['preproc__feats__scaled_fft__fft__is_concerned'] = df['preproc__feats__scaled_fft__fft__is_concerned'].apply(lambda x: [i for i, y in enumerate(uniques) if x == y][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/home/jim/PycharmProjects/seminar/notebooks/grid_search_all_gts_yearly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_feats_cols = [col for col in df if col.startswith('preproc__feats__scaled_ext')]\n",
    "\n",
    "figs = []\n",
    "for val in ['adjusted_mutual_info_score']:\n",
    "    figs.append(df.hvplot.scatter('clustering__n_clusters', val, by=['gt']))\n",
    "hv.Overlay(figs).opts(width=1200, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/jim/PycharmProjects/seminar/notebooks/grid_search_all_gts_yearly.csv')\n",
    "df.sort_values('adjusted_mutual_info_score', ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit in total extent in 2017 (or any other year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = None #2017\n",
    "ground_truth_depth_non_wetland = 1\n",
    "ground_truth_depth_wetland = 1\n",
    "mask = None #pd.DataFrame(arr_wm_all.data.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, gt_samples, sample_index = sample(100000, ground_truth=only_valid_gt, mask=mask)\n",
    "\n",
    "if year is not None:\n",
    "    samples = filter_time(InputList(samples), year=year)\n",
    "else:\n",
    "    samples = InputList(samples)\n",
    "    \n",
    "gtt_samples = reduce_gt(gt_samples, ground_truth_depth_wetland, ground_truth_depth_non_wetland)\n",
    "\n",
    "gtr = reduce_gt(aligned_gt.data, ground_truth_depth_wetland, ground_truth_depth_non_wetland)\n",
    "\n",
    "gtr = xr.DataArray(gtr, dims=['band', 'y', 'x'])\n",
    "gtr.attrs = aligned_gt.attrs\n",
    "gtr = gtr.assign_coords(aligned_gt.coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, gtt_samples, sample_index = sample(100000, ground_truth=only_valid_gt_reduced, mask=mask)\n",
    "\n",
    "if year is not None:\n",
    "    samples = filter_time(InputList(samples), year=year)\n",
    "else:\n",
    "    samples = InputList(samples)\n",
    "    \n",
    "gtt_samples = gtt_samples\n",
    "gtr = aligned_gt_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set, label_counts = np.unique(gtr.data, return_counts=True)\n",
    "\n",
    "label_to_idx = dict(zip(label_set, range(len(label_set))))\n",
    "label_to_idx[-1] = -1\n",
    "mapper_gtr = np.vectorize(lambda ind: label_to_idx.get(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is_concerned = [False, True] + [False] * len(arrs_opt_names) + [False] * len(indices)\n",
    "#is_concerned_ext = [True, True] + [True, True, True] + [False]* (len(arrs_opt_names) - 3) + [True] * len(indices)\n",
    "is_conc_fft_vv_vh = [True, True] + [False] * len(indices) + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_vv_vh_evi = [True, True] + [True, False, False, False, False, False, False] + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_vh = [False, True] + [False] * len(indices) + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_vv = [True, False] + [False] * len(indices) + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_evi = [False, False, True] + [False] * (len(indices) - 1) + [False] * len(arrs_tex_names)\n",
    "is_conc_fft_none = [False, False, False] + [False] * (len(indices) - 1) + [False] * len(arrs_tex_names)\n",
    "\n",
    "is_concerned = is_conc_fft_vv_vh_evi\n",
    "is_concerned_ext = [True, True] + [True] * len(indices) + [True] * len(arrs_tex_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_fft = Pipeline([('fft', FFCombo(len_inp=samples.nr_inputs, is_concerned=is_concerned,\n",
    "                                              scaler=std_norm, thr_freq=8, scale=True, quantile_range=(0.25, 0.75), \n",
    "                                              ordered_tkwargs=ordered_tkwargs_fft))\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_other = Pipeline([('preproc', \n",
    "                       DynamicFeatureUnion([\n",
    "                           ('min', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=samples.nr_inputs, \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.min, axis=1))], is_on=1)),\n",
    "                            \n",
    "                           ('max', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=samples.nr_inputs, \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.max, axis=1))], is_on=1)),\n",
    "                           \n",
    "                           ('mean', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=samples.nr_inputs, \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.median, axis=1))], is_on=1)),\n",
    "                           ('q25', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=samples.nr_inputs, \n",
    "                                                                                        transformer=get_scaled_func_transformer(partial(np.quantile, q=0.25), axis=1))], is_on=1)),\n",
    "                           ('q75', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=samples.nr_inputs, \n",
    "                                                                                        transformer=get_scaled_func_transformer(partial(np.quantile, q=0.75), axis=1))], is_on=1)),\n",
    "                           ('std', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=samples.nr_inputs, \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.std, axis=1))], is_on=1)),\n",
    "                           ('median', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=samples.nr_inputs, \n",
    "                                                                                        transformer=get_scaled_func_transformer(np.mean, axis=1))], is_on=1)),\n",
    "                           ('diff_suwi_mean', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=samples.nr_inputs, \n",
    "                                                                                        transformer=get_scaled_func_transformer(diff_suwi, reducer=np.mean))], is_on=1)),\n",
    "                           ('diff_suwi_std', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=samples.nr_inputs, \n",
    "                                                                                        transformer=get_scaled_func_transformer(diff_suwi, reducer=np.std))], is_on=0)),\n",
    "                           ('diff_suwi_q75', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=samples.nr_inputs, \n",
    "                                                                                        transformer=get_scaled_func_transformer(diff_suwi, reducer=partial(np.quantile, q=0.75)))], is_on=1)),\n",
    "                           ('diff_suwi_q25', TransformerSwitch(transformers=[None, \n",
    "                                                                   FuncTransformerCombo(is_concerned=is_concerned_ext, len_inp=samples.nr_inputs, \n",
    "                                                                                        transformer=get_scaled_func_transformer(diff_suwi, reducer=partial(np.quantile, q=0.25)))], is_on=1))\n",
    "                     ]))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Pipeline([#('input_format', FunctionTransformer(to_input_list)), \n",
    "                    ('scale', normer),\n",
    "                    #('pca', TransformerSwitch(transformers=[None,\n",
    "                    #                                        PCA(n_components=40)], is_on=0)),\n",
    "                    ('feats', FeatureUnion([('scaled_fft', pip_fft),\n",
    "                                            ('scaled_ext', pip_other)\n",
    "                                           ]))\n",
    "                    #('scaled_ext', pip_fft)\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_ext = Pipeline([('preproc', preproc),\n",
    "                    ('pca', TransformerSwitch(transformers=[None, PCA(n_components=10)], is_on=False, transparent=True)), \n",
    "                    ('clustering', TransformerSwitch(transformers=[cl.KMeans(n_clusters=30, n_jobs=12),\n",
    "                                                                   #cl.AffinityPropagation(max_iter=500),\n",
    "                                                                   GaussianMixture(n_components=30,),\n",
    "                                                                   #cl.AgglomerativeClustering(n_clusters=10, linkage='ward'),\n",
    "                                                                   #cl.MeanShift(max_iter=500, n_jobs=12)\n",
    "                                                                   RandomForestClassifier(max_depth=5, n_estimators=100, max_features=1)\n",
    "                                                                  ], is_on=-0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip, assign, sa_out = cluster_based_classif(samples, gtt_samples, pip_ext, n_clusters=pip_ext['clustering'].get().n_clusters, score=f1_score, train=True, freq_weighted=True,\n",
    "                                    annealing_params=dict(epochs=200, T=5e-3, eta=0.99, max_change=1, verbose=True, sample=10, subset=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot([h[1].mean() for h in sa_out['hist']])\n",
    "plt.ylabel(r'mean of f1 scores')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.gcf().set_figwidth(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pip = pip_ext\n",
    "a = pip['clustering'].get().cluster_centers_\n",
    "#a = pip_ext['clustering'].get().means_\n",
    "\n",
    "inds = (pip_fft['fft'].get_feature_names() + \\\n",
    "              ['mean %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['mean'] is not None] + \n",
    "              ['min %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['min'] is not None] + \n",
    "              ['max %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['max'] is not None] + \n",
    "              ['median %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i]] + \\\n",
    "              ['q25 %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['q25'] is not None] + \n",
    "              ['q75 %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['q75'] is not None] + \n",
    "              ['std %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['std'] is not None] +\n",
    "              ['diff_suwi_mean %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['diff_suwi_mean'] is not None] +\n",
    "              ['diff_suwi_std %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['diff_suwi_std'] is not None] +\n",
    "              ['diff_suwi_q75 %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['diff_suwi_q75'] is not None] +\n",
    "              ['diff_suwi_q25 %s' % n for i, n in enumerate(all_names) if is_concerned_ext[i] and dict(pip_other['preproc'].transformer_list)['diff_suwi_q25'] is not None]\n",
    "       )\n",
    "pd.DataFrame(a.transpose(), index=inds).hvplot(rot=90, width=1700, height=400, cmap=colormap) #cmap=['#ffffff'] + palettes.Category10[8]) #, xformatter='%.3f')#.opts(colorbar=True, cmap='paired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mask is not None:\n",
    "    inp = mask_dframe_with_dframe(InputList(only_valid_dfxx_unnorm), pd.DataFrame(arr_wm_all.data.flatten()))\n",
    "else:\n",
    "    inp = InputList(only_valid_dfxx_unnorm)\n",
    "    \n",
    "if year is not None:\n",
    "    inp = filter_time(inp, year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf, (valids, conf) = train_predict_conf(pip=pip['pip'], \n",
    "                                          dset=inp, \n",
    "                                          model_arr=arrs_vv[0, :1], \n",
    "                                          ground_truth=gtr,\n",
    "                                          classif_out_path='/home/jim/PycharmProjects/seminar/notebooks/data/preds/all_time__all_space__200cl__gtt33',\n",
    "                                          samples=samples,\n",
    "                                          is_aligned=True,\n",
    "                                          load_classif=False,\n",
    "                                          gt=None,\n",
    "                                          train=False,\n",
    "                                          gt_nan=-1,\n",
    "                                          pred_nan=-1, \n",
    "                                          n_jobs=8, chunks=1, verbose=10, n_batches=300, joblib=True)\n",
    "\n",
    "clf.data = clf.data.astype(np.int)\n",
    "clf.data[np.where(clf.data == 65535)] = -1\n",
    "\n",
    "assignment_to_label = dict(zip(range(len(assign)), assign))\n",
    "assignment_to_label[-1] = -1  \n",
    "mapper = np.vectorize(lambda x: assignment_to_label.get(x, None))\n",
    "\n",
    "clf = clf.copy()\n",
    "clf.data = mapper(clf.data)\n",
    "\n",
    "labels = np.unique(gtr.data)[1:]\n",
    "prec, reca, f1 = precision_score(gtr.data[valids], clf.data[valids], average=None), recall_score(gtr.data[valids], clf.data[valids], average=None), f1_score(gtr.data[valids], clf.data[valids], average=None)\n",
    "#time_series_results[year] = (clf, valids, conf, prec, reca, f1, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = arr_wm_all.data\n",
    "    \n",
    "mask = np.logical_and(np.logical_and(mask, clf.data != -1), gtr.data != -1)\n",
    "gtrm = gtr.data[mask].flatten()\n",
    "clfm = clf.data[mask].flatten()\n",
    "\n",
    "f1m_all = f1_score(gtrm, clfm, average=None).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_all = f1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(gtr.data)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clffn_colors = 100\n",
    "#colorset = list(map(str, range(10))) + ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "#colormap = ['#' + ''.join(map(str, np.random.choice(colorset, 6))) for i in range(n_colors)]\n",
    "\n",
    "#cmap = {i: c for i, c in enumerate(colormap)}\n",
    "#cmap[-1] = '#ffffff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colormap[0] = '#ffe356'\n",
    "#colormap[1] = '#ab1243'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "labels = np.unique(gtr.data)[1:]\n",
    "labels += 300\n",
    "\n",
    "\n",
    "raw_cmap = [colors.to_rgb(colors.hex2color(c)) for c in colormap]\n",
    "cmap = colors.ListedColormap(['#ffffff'] + raw_cmap)\n",
    "norm = colors.BoundaryNorm(np.r_[-1, labels], ncolors=len(labels))\n",
    "\n",
    "_ = plt.matshow(np.atleast_2d(np.r_[-1, labels]), cmap=cmap, norm=norm)\n",
    "\n",
    "_ = plt.xticks(range(len(labels) + 1), np.r_[-1, labels-300])\n",
    "_ = plt.yticks([], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.unique(gtr.data)[1:]\n",
    "label_to_idx = dict(zip(labels, range(len(labels))))\n",
    "label_to_idx[-1] = -1\n",
    "mapper_gtr = np.vectorize(lambda ind: label_to_idx.get(ind, -1))\n",
    "\n",
    "clff = clf.copy()\n",
    "clff.data = mapper_gtr(clff.data)\n",
    "\n",
    "uniques = np.unique(clff.data)\n",
    "cmap = list(np.array(colormap)[uniques])\n",
    "\n",
    "label_to_idx2 = dict(zip(uniques, range(len(uniques))))\n",
    "mapper_gtr2 = np.vectorize(lambda ind: label_to_idx2.get(ind, -1))\n",
    "\n",
    "clff.data = mapper_gtr2(clff.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(clff.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clff.hvplot.image('x', 'y', rasterize=False, height=1200, width=1200, \n",
    "                       cmap=cmap, colorbar=False)# cmap='viridis', # cmap=['#ffffff'] + palettes.Category10[9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 20))\n",
    "plot_conf(conf, n_clusters=30, n_classes=10, labels = np.unique(gtr.data)[1:])\n",
    "plt.gcf().set_figheight(17)\n",
    "plt.gcf().set_figwidth(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_results_no_fft = {}\n",
    "time_series_results_no_fft[2016] = clf, valids, conf, prec, reca, f1, labels, copy.deepcopy(pip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_results_gt11 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, year in enumerate([2016, 2017, 2018]):\n",
    "    col = np.where(pd.to_datetime(arrs_wm_yearly_max.time.data).year == year)[0]\n",
    "    #inp = mask_dframe_with_dframe(InputList(only_valid_dfxx_unnorm), pd.DataFrame(arrs_wm_yearly_max[col].data.flatten()))\n",
    "    samples_year = filter_time(samples, year=year)\n",
    "    inp = filter_time(InputList(only_valid_dfxx_unnorm), year=year)\n",
    "    \n",
    "    pip, assign, sa_out = cluster_based_classif(samples_year, gtt_samples, pip_ext, n_clusters=pip_ext['clustering'].get().n_clusters, score=f1_score, train=True, freq_weighted=True,\n",
    "                                    annealing_params=dict(epochs=200, T=5e-3, eta=0.99, max_change=1, verbose=True, sample=10, subset=1))\n",
    "\n",
    "    clf, (valids, conf) = train_predict_conf(pip=pip['pip'], \n",
    "                                              dset=inp, \n",
    "                                              model_arr=arrs_vv[0, :1], \n",
    "                                              ground_truth=gtr,\n",
    "                                              classif_out_path='/home/jim/PycharmProjects/seminar/notebooks/data/clustered_year_%d.tif' % year,\n",
    "                                              samples=None,\n",
    "                                              is_aligned=True,\n",
    "                                              load_classif=False,\n",
    "                                              gt=None,\n",
    "                                              train=False,\n",
    "                                              gt_nan=-1,\n",
    "                                              pred_nan=-1, \n",
    "                                              n_jobs=8, chunks=1, verbose=10, n_batches=300, joblib=True)\n",
    "    \n",
    "    clf.data = clf.data.astype(np.int)\n",
    "    clf.data[np.where(clf.data == 65535)] = -1\n",
    "\n",
    "    assignment_to_label = dict(zip(range(len(assign)), assign))\n",
    "    assignment_to_label[-1] = -1  \n",
    "    mapper = np.vectorize(lambda x: assignment_to_label.get(x, None))\n",
    "\n",
    "    clf = clf.copy()\n",
    "    clf.data = mapper(clf.data)\n",
    "\n",
    "    labels = np.unique(gtr.data)[1:]\n",
    "    prec, reca, f1 = precision_score(gtr.data[valids], clf.data[valids], average=None), recall_score(gtr.data[valids], clf.data[valids], average=None), f1_score(gtr.data[valids], clf.data[valids], average=None)\n",
    "    time_series_results_gt11[year] = (clf, valids, conf, prec, reca, f1, labels, copy.deepcopy(pip))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1m = []\n",
    "f1s = []\n",
    "\n",
    "precs = []\n",
    "precm = []\n",
    "\n",
    "recas = []\n",
    "recam = []\n",
    "ts = time_series_results_gt11\n",
    "\n",
    "for i, year in enumerate([2016, 2017, 2018]):\n",
    "    clf = ts[year][0].copy()\n",
    "    mask = arrs_wm_yearly_min[i].data\n",
    "    \n",
    "    mask = np.logical_and(np.logical_and(mask, clf.data != -1), gtr.data != -1)\n",
    "    gtrm = gtr.data[mask].flatten()\n",
    "    clfm = clf.data[mask].flatten()\n",
    "    \n",
    "    f1 = f1_score(gtrm, clfm, average=None)\n",
    "    f1m.append(f1)\n",
    "    f1s.append(ts[year][5])\n",
    "    \n",
    "    prec = precision_score(gtrm, clfm, average=None)\n",
    "    precm.append(prec)\n",
    "    precs.append(ts[year][4])\n",
    "    \n",
    "    reca = recall_score(gtrm, clfm, average=None)\n",
    "    recam.append(reca)\n",
    "    recas.append(ts[year][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "for i, year in enumerate([2016, 2017, 2018]):\n",
    "    res_year = pd.DataFrame({'OPT-YR %d MASKED' % year: f1m[i], 'OPT-YR %d UNMASKED' % year: f1s[i]})\n",
    "    res = pd.concat([res, res_year], axis=1)\n",
    "res = pd.concat([res, pd.DataFrame({'OPT UNMASKED': f1_all, 'OPT MASKED': f1m_all})], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels = np.array([1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 14])\n",
    "#cols = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "labels = [1, 2, 3, 4, 5]\n",
    "\n",
    "res.loc[:, 'label'] = labels\n",
    "res = res.set_index('label')\n",
    "res.loc[:, ['OPT-YR 2016 UNMASKED', 'OPT-YR 2017 UNMASKED', 'OPT-YR 2018 UNMASKED', 'OPT UNMASKED', \n",
    "            'OPT-YR 2016 MASKED', 'OPT-YR 2017 MASKED', 'OPT-YR 2018 MASKED', 'OPT MASKED']].hvplot.bar(width=1500, height=400, rot=90, ylim=(0, 1), ylabel='f1', xlabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels = np.array([1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12, 14])\n",
    "for i, year in enumerate([2016, 2017, 2018]):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    ax1 = plt.subplot(311)\n",
    "    ax1.set_title('OPT-YR %d' % year)\n",
    "    ax1.bar(x=labels-0.2, height=f1s[i], width=0.3, align='center', alpha=1)\n",
    "    ax1.bar(x=labels+0.2, height=f1m[i], width=0.3, align='center', alpha=1)\n",
    "    ax1.set_ylabel('f1')\n",
    "    ax1.set_xlabel('class')\n",
    "    \n",
    "    ax2 = plt.subplot(312)\n",
    "    ax2.set_title('OPT-YR %d' % year)\n",
    "    ax2.bar(x=labels-0.2, height=precs[i], width=0.3, align='center', alpha=1)\n",
    "    ax2.bar(x=labels+0.2, height=precm[i], width=0.3, align='center', alpha=1)\n",
    "    ax2.set_ylabel('f1')\n",
    "    ax2.set_xlabel('class')\n",
    "    \n",
    "    ax3 = plt.subplot(313)\n",
    "    ax3.set_title('OPT-YR %d' % year)\n",
    "    ax3.bar(x=labels-0.2, height=recas[i], width=0.3, align='center', alpha=1)\n",
    "    ax3.bar(x=labels+0.2, height=recam[i], width=0.3, align='center', alpha=1)\n",
    "    ax3.set_ylabel('f1')\n",
    "    ax3.set_xlabel('class')\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.xticks(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
